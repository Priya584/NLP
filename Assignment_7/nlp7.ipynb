{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ELhOE5S-5uP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7beddc1b-24eb-4651-c7d2-40d2818bec45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading and Splitting Data ---\n",
            "Data loaded and split successfully:\n",
            "Total Sentences: 100000\n",
            "Training set:   80000 sentences\n",
            "Validation set: 10000 sentences\n",
            "Test set:       10000 sentences\n",
            "\n",
            "--- Loading Probability Files ---\n",
            "\n",
            "--- 1. Starting Task 1: PMI Scores ---\n",
            "Loaded 138132 unigram and 822206 bigram probabilities.\n",
            "Calculated PMI for 106764 unique bigrams in validation set.\n",
            "Calculated PMI for 106090 unique bigrams in test set.\n",
            "\n",
            "--- Finished Task 1 ---\n",
            "\n",
            "--- 2. Starting Task 2: TF-IDF Vectorization ---\n",
            "Fitting TF-IDF on 80000 training sentences...\n",
            "Transforming 10000 validation sentences...\n",
            "Transforming 10000 testing sentences...\n",
            "\n",
            "TF-IDF Matrix Shapes:\n",
            "Train: (80000, 120665)\n",
            "Val:   (10000, 120665)\n",
            "Test:  (10000, 120665)\n",
            "Vocabulary size (from train): 120665\n",
            "--- Finished Task 2 ---\n",
            "\n",
            "--- 3. Starting Task 3: Nearest Neighbors ---\n",
            "--- Finding neighbors for Validation Set (within val) ---\n",
            "\n",
            "Found nearest neighbors. Sample results:\n",
            "-------------------------\n",
            "Original (idx 0): ркЖ ркЖркзрлБркирк┐ркХ рк╣рлЛрк╕рлНрккрк┐ркЯрк▓ ркЖркЬрлЗ ркЦрк╛рк▓рлА ркмрлЗркбрлН рк╕ ркЕркирлЗ рк╕ркЬрк╛рк╡ркЯрлА рк╕рк╛ркоркЧрлНрк░рлА рк╕рк╛ркерлЗ ркПркХ рклрк┐рк▓рлНрко рк╕рлНркЯрлБркбрк┐ркпрлЛркирлА ркЬрлЗрко ркЙркнрлБ ркЫрлЗ .\n",
            "Neighbor (idx 3839): рк╕ .\n",
            "Cosine Distance: 0.7046\n",
            "-------------------------\n",
            "Original (idx 1): ркжрк╣рлЗркЬркирк╛ рлк ркЕркЧрлНрк░ркгрлАркУ ркбрк┐ркЯрлЗркЗрки ркЖрк╢рк╛ рк╡рк░рлНркХрк░ ркмрк╣рлЗркирлЛркирлЛ рк╣ркВркЧрк╛ркорлЛ\n",
            "Neighbor (idx 9837): ркПркХ рк╕рлЛрк╢рк┐ркпрк▓ рк╡рк░рлНркХрк░ рк╣рлЛрк╡рк╛ ркЫркдрк╛ркВ рккркг ркПркгрлЗ ркЖркЯрк▓рлБркВ ркЦрк░рк╛ркм ркХрк╛рко ркХрк░рлНркпрлБ .\n",
            "Cosine Distance: 0.8233\n",
            "-------------------------\n",
            "Original (idx 2): ркПрк╡рлА рк╕рлНркерк┐ркдрк┐ркорк╛ркВ ркбрлНрк░рлЛрккрк▓рлЗркЯркирлЛ рк▓рлАркХрлЗркЬ ркеркИ рк╢ркХрлЗ ркЫрлЗ ркЬрлЗ ркХрлЛрк░рлЛркирк╛ркирлБ ркХрк╛рк░ркг ркмркирлА рк╢ркХрлЗ ркЫрлЗ .\n",
            "Neighbor (idx 5162): ркЪрк╛ рк╡рк╛рк│рк╛ рк╡ркбрк╛рккрлНрк░ркзрк╛рки ркмркирлА рк╢ркХрлЗ , ркорк╛ркЫрлАркорк╛рк░ркирк╛ ркжрлАркХрк░рк╛ рк╡рлИркЬрлНркЮрк╛ркирк┐ркХ ркмркирлА рк╢ркХрлЗ ркдрлЛ ркирлЛркХрк░рк╛ркгрлАркирлЛ ркжрк┐ркХрк░рлЛ ркЬркЬ рки ркмркирлА рк╢ркХрлЗ ?\n",
            "Cosine Distance: 0.7121\n",
            "-------------------------\n",
            "Original (idx 3): 4 рк╕рк╛ркоркЧрлНрк░рлА :\n",
            "Neighbor (idx 7927): ркЖркзрлБркирк┐ркХ рк╕рк╛ркзркиркирлА рккрлЗркирк▓ , 4 рк╕рлНрккрлЛркХ рк╕рлНркЯрлАркпрк░рлАркВркЧ рк╡рлНрк╣рлАрк▓ ркЖрк╡рлА рк╣ркдрлА , ркЕркирлЗ рк╕рк╛ркоркЧрлНрк░рлА рк╡ркзрлБ рк╕рк╛рк░рлА ркЕркирлЗ ркЙркЪрлНркЪ ркЧрлБркгрк╡ркдрлНркдрк╛ ркЫрлЗ .\n",
            "Cosine Distance: 0.7124\n",
            "-------------------------\n",
            "Original (idx 4): ркЬрлЛ ркдрлЗркУ ркЖрк╡рлЗ ркдрлЛ ркорк╣рк┐рк▓рк╛ркУркирлЗ рк╡рк╛рк╕ркгрлЛ рк╡ркбрлЗ ркорк╛рк░ ркорк╛рк░рк╢рлЗ .\n",
            "Neighbor (idx 6436): ркХркВркХрлЛркбрк╛ркирлЗ ркХркВркж рк╡ркбрлЗ ркЙркЫрлЗрк░рлА рк╢ркХрк╛ркп ркЫрлЗ .\n",
            "Cosine Distance: 0.8102\n",
            "\n",
            "--- Finding neighbors for Test Set (within test) ---\n",
            "\n",
            "Found nearest neighbors. Sample results:\n",
            "-------------------------\n",
            "Original (idx 0): ркзрлАркорлА ркзрк╛рк░рлЗ рк╡рк░рк╕рк╛ркж рк╡рк░рк╕рлНркпрлЛ рк╣ркдрлЛ .\n",
            "Neighbor (idx 3119): рк░рк╛ркЬркХрлЛркЯ рк╕рк╣рк┐ркд рк╕рлМрк░рк╛рк╖рлНркЯрлНрк░-ркХркЪрлНркЫркорк╛ркВ рк╡рк╛ркпрлБ рк╡рк╛рк╡рк╛ркЭрлЛркбрк╛ркирлА ркЕрк╕рк░ ркеркдрк╛ ркЕркирлЗркХ ркЬркЧрлНркпрк╛ркП ркЭрк╛рккркЯрк╛ркерлА ркорк╛ркВркбрлАркирлЗ рлзрлз ркИркВркЪ ркЬрлЗркЯрк▓рлЛ рк╡рк░рк╕рк╛ркж рк╡рк░рк╕рлНркпрлЛ рк╣ркдрлЛ .\n",
            "Cosine Distance: 0.7457\n",
            "-------------------------\n",
            "Original (idx 1): ркорлГркдркХрлЛркирлА рк╕ркВркЦрлНркпрк╛ 6 ркЫрлЗ .\n",
            "Neighbor (idx 6106): рклрлНрк░рк╛ркВрк╕ркорк╛ркВ ркорлГркдркХрлЛркирлА рк╕ркВркЦрлНркпрк╛ 12200 ркерлА рк╡ркзрлБ ркЫрлЗ .\n",
            "Cosine Distance: 0.4009\n",
            "-------------------------\n",
            "Original (idx 2): рк╡рк╣рлБ ркРрк╢рлНрк╡рк░рлНркпрк╛ркирлЗ рк▓ркИркирлЗ ркХрлЛркИркП ркХрк░рлА ркнркжрлНркжрлА ркХркорлЗркирлНркЯ , ркЕркорк┐ркдрк╛ркнрлЗ рклркХрлНркд ркПркХ ркЬрк╡рк╛ркм ркЖрккрлАркирлЗ ркХрк░рлА ркжрлАркзрлА ркмрлЛрк▓ркдрлА ркмркВркз\n",
            "Neighbor (idx 6254): ркЖрк╡рлА рк░рк╣рлНркпрк╛ркВ ркЫрлЗ ркХркорлЗркирлНркЯ\n",
            "Cosine Distance: 0.7501\n",
            "-------------------------\n",
            "Original (idx 3): ркЦркЯрк╛ркгрк╛ ркЕркирлЗ ркЕркзрк┐ркХ ркХрк▓рлЗркХркЯрк░ ркХрлЗркдрки ркЬрлЛрк╢рлА рк╕рк╣рк┐ркдркирк╛ ркЕркзрк┐ркХрк╛рк░рлАркУ ркдрлЗркоркЬ ркЖркЧрлЗрк╡рк╛ркирлЛркирлА рккрлНрк░рлЗрк░ркХ ркЙрккрк╕рлНркерк┐ркдрк┐ рк░рк╣рлА рк╣ркдрлА .\n",
            "Neighbor (idx 2416): ркжрк┐ркирлЗрк╢ ркЬрлЛрк╢рлА , ркпрлБ .\n",
            "Cosine Distance: 0.8238\n",
            "-------------------------\n",
            "Original (idx 4): .\n",
            "Neighbor (idx 3619): .\n",
            "Cosine Distance: 0.0000\n",
            "\n",
            "--- Finished Task 3 ---\n",
            "\n",
            "Assignment Complete.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_and_split_data(fname, col_idx, val_sz=0.1, tst_sz=0.1):\n",
        "    \"\"\"\n",
        "    Loads data from a single Parquet file and splits it into train, val, and test.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the Parquet file\n",
        "        df = pd.read_parquet(fname)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"---\" * 20)\n",
        "        print(f\"ЁЯЪи ERROR: Main data file not found: '{fname}'\")\n",
        "        print(f\"Please update the 'TOKENIZED_FILE' variable.\")\n",
        "        print(f\"---\" * 20)\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        # Get sentences from the first column (index 0)\n",
        "        sents = df.iloc[:, col_idx].dropna().tolist()\n",
        "    except IndexError:\n",
        "        print(f\"---\" * 20)\n",
        "        print(f\"ЁЯЪи ERROR: Column index {col_idx} is out of bounds.\")\n",
        "        print(f\"Please update the 'TEXT_COLUMN_INDEX' variable.\")\n",
        "        print(f\"---\" * 20)\n",
        "        return None, None, None\n",
        "\n",
        "    # --- Create Splits ---\n",
        "    train_split = 1.0 - (val_sz + tst_sz) # 1.0 - 0.2 = 0.8\n",
        "    trn_s, tmp_s = train_test_split(sents, train_size=train_split, random_state=42)\n",
        "\n",
        "    test_split_rel = tst_sz / (val_sz + tst_sz) # 0.10 / (0.10 + 0.10) = 0.5\n",
        "    val_s, tst_s = train_test_split(tmp_s, test_size=test_split_rel, random_state=42)\n",
        "\n",
        "    print(f\"Data loaded and split successfully:\")\n",
        "    print(f\"Total Sentences: {len(sents)}\")\n",
        "    print(f\"Training set:   {len(trn_s)} sentences\")\n",
        "    print(f\"Validation set: {len(val_s)} sentences\")\n",
        "    print(f\"Test set:       {len(tst_s)} sentences\")\n",
        "\n",
        "    return trn_s, val_s, tst_s\n",
        "\n",
        "def calc_pmi(w1, w2, u_p, b_p):\n",
        "    \"\"\"\n",
        "    Calculates PMI(w1, w2) = log2( P(w2 | w1) / P(w2) )\n",
        "    \"\"\"\n",
        "    p_cond = b_p.get((w1, w2)) # P(w2 | w1)\n",
        "    p_w2 = u_p.get(w2)       # P(w2)\n",
        "\n",
        "    if p_cond is None or p_w2 is None or p_w2 == 0 or p_cond == 0:\n",
        "        return -np.inf\n",
        "\n",
        "    pmi = math.log2(p_cond / p_w2)\n",
        "    return pmi\n",
        "\n",
        "def get_pmi_for_set(sents, u_p, b_p):\n",
        "    \"\"\"Calculates PMI for all unique bigrams in a list of sentences.\"\"\"\n",
        "    all_pmi = {}\n",
        "    for s in sents:\n",
        "        # Your sentences look pre-tokenized with spaces, so .split() is correct\n",
        "        words = s.split()\n",
        "        if len(words) < 2:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(words) - 1):\n",
        "            w1 = words[i]\n",
        "            w2 = words[i+1]\n",
        "            bg = (w1, w2)\n",
        "\n",
        "            if bg not in all_pmi:\n",
        "                all_pmi[bg] = calc_pmi(w1, w2, u_p, b_p)\n",
        "    return all_pmi\n",
        "\n",
        "def find_nn(X_data, sents):\n",
        "    \"\"\"Finds the nearest neighbor for each item in X_data within itself.\"\"\"\n",
        "    nn = NearestNeighbors(n_neighbors=2, metric='cosine', algorithm='brute')\n",
        "    nn.fit(X_data)\n",
        "\n",
        "    d, idx = nn.kneighbors(X_data)\n",
        "\n",
        "    print(f\"\\nFound nearest neighbors. Sample results:\")\n",
        "\n",
        "    for i in range(min(5, len(sents))):\n",
        "        orig_idx = i\n",
        "        nn_idx = idx[i][1]   # Get the *second* index (the first is the item itself)\n",
        "        nn_dist = d[i][1] # Get the *second* distance\n",
        "\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"Original (idx {orig_idx}): {sents[orig_idx]}\")\n",
        "        print(f\"Neighbor (idx {nn_idx}): {sents[nn_idx]}\")\n",
        "        print(f\"Cosine Distance: {nn_dist:.4f}\")\n",
        "\n",
        "    return idx, d\n",
        "\n",
        "def main():\n",
        "    # --- тЪая╕П ACTION REQUIRED: Update these variables ---\n",
        "\n",
        "    # 1. Your Parquet file with all sentences\n",
        "    TOKENIZED_FILE = 'tokenized_gujarati_sentences.parquet'\n",
        "\n",
        "    # 2. The *index* of the column with sentences (0 for the first column)\n",
        "    TEXT_COLUMN_INDEX = 0\n",
        "\n",
        "    # 3. Your probability files (from your first image)\n",
        "    UNIGRAM_CSV = 'unigram_probs.csv'\n",
        "    BIGRAM_CSV = 'bigram_probs.csv'\n",
        "\n",
        "    # 4. Define split sizes (80% train, 10% val, 10% test)\n",
        "    VAL_SIZE = 0.10\n",
        "    TEST_SIZE = 0.10\n",
        "    # --- ------------------------------------------ ---\n",
        "\n",
        "    # --- Load and Split Data ---\n",
        "    print(\"--- Loading and Splitting Data ---\")\n",
        "    trn_s, val_s, tst_s = load_and_split_data(\n",
        "        TOKENIZED_FILE, TEXT_COLUMN_INDEX, VAL_SIZE, TEST_SIZE\n",
        "    )\n",
        "\n",
        "    if not trn_s:\n",
        "        print(\"ЁЯЪи Stopping execution due to data loading error.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Loading Probability Files ---\")\n",
        "    try:\n",
        "        u_df = pd.read_csv(UNIGRAM_CSV)\n",
        "        b_df = pd.read_csv(BIGRAM_CSV)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"ЁЯЪи ERROR: Probability file not found: {e.fileName}\")\n",
        "        print(f\"Please make sure '{UNIGRAM_CSV}' and '{BIGRAM_CSV}' are present.\")\n",
        "        return\n",
        "\n",
        "    # --- Task 1: PMI Scores ---\n",
        "    print(\"\\n--- 1. Starting Task 1: PMI Scores ---\")\n",
        "\n",
        "    u_df = u_df.dropna(subset=['Word'])\n",
        "    b_df = b_df.dropna(subset=['Context', 'Word'])\n",
        "\n",
        "    u_p = u_df.set_index('Word')['Probability'].to_dict()\n",
        "    b_p = b_df.set_index(['Context', 'Word'])['Probability'].to_dict()\n",
        "\n",
        "    print(f\"Loaded {len(u_p)} unigram and {len(b_p)} bigram probabilities.\")\n",
        "\n",
        "    val_pmi = get_pmi_for_set(val_s, u_p, b_p)\n",
        "    test_pmi = get_pmi_for_set(tst_s, u_p, b_p)\n",
        "\n",
        "    print(f\"Calculated PMI for {len(val_pmi)} unique bigrams in validation set.\")\n",
        "    print(f\"Calculated PMI for {len(test_pmi)} unique bigrams in test set.\")\n",
        "\n",
        "    print(\"\\n--- Finished Task 1 ---\")\n",
        "\n",
        "    # --- Task 2: TF-IDF Vectorization ---\n",
        "    print(\"\\n--- 2. Starting Task 2: TF-IDF Vectorization ---\")\n",
        "\n",
        "    vec = TfidfVectorizer(\n",
        "        tokenizer=lambda x: x.split(),\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "\n",
        "    print(f\"Fitting TF-IDF on {len(trn_s)} training sentences...\")\n",
        "    X_trn = vec.fit_transform(trn_s)\n",
        "\n",
        "    print(f\"Transforming {len(val_s)} validation sentences...\")\n",
        "    X_val = vec.transform(val_s)\n",
        "\n",
        "    print(f\"Transforming {len(tst_s)} testing sentences...\")\n",
        "    X_tst = vec.transform(tst_s)\n",
        "\n",
        "    print(f\"\\nTF-IDF Matrix Shapes:\")\n",
        "    print(f\"Train: {X_trn.shape}\")\n",
        "    print(f\"Val:   {X_val.shape}\")\n",
        "    print(f\"Test:  {X_tst.shape}\")\n",
        "    print(f\"Vocabulary size (from train): {len(vec.vocabulary_)}\")\n",
        "    print(\"--- Finished Task 2 ---\")\n",
        "\n",
        "    # --- Task 3: Nearest Neighbors ---\n",
        "    print(\"\\n--- 3. Starting Task 3: Nearest Neighbors ---\")\n",
        "\n",
        "    print(\"--- Finding neighbors for Validation Set (within val) ---\")\n",
        "    val_nn_idxs, val_nn_dists = find_nn(X_val, val_s)\n",
        "\n",
        "    print(\"\\n--- Finding neighbors for Test Set (within test) ---\")\n",
        "    test_nn_idxs, test_nn_dists = find_nn(X_tst, tst_s)\n",
        "\n",
        "    print(\"\\n--- Finished Task 3 ---\")\n",
        "    print(\"\\nAssignment Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}