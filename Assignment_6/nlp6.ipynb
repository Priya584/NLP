{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "griKys-nT1po",
        "outputId": "3bc85a20-1a3f-4b71-96bf-633e93c9fcff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded unigram_probs.csv (1 contexts)\n",
            "Loaded bigram_probs.csv (138132 contexts)\n",
            "Loaded trigram_probs.csv (817679 contexts)\n",
            "Loaded quadrigram_probs.csv (1223605 contexts)\n",
            "Total unique contexts in combined model: 2179416\n",
            "P(word='આ' | context='<s> <s> <s>') = 0.07077\n",
            "P(word='છે' | context='આ એક') = 0.0083333333333333\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# --- NEW LOADING LOGIC ---\n",
        "\n",
        "def load_model_file(fname):\n",
        "    \"\"\"\n",
        "    Loads an n-gram CSV file into a dictionary.\n",
        "    Maps unigrams to the context ('<s>',) to match the model's logic.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(fname)\n",
        "    p = defaultdict(dict)\n",
        "\n",
        "    # Check if 'Context' column exists.\n",
        "    has_c = 'Context' in df.columns\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        w = row[\"Word\"]\n",
        "        p_val = row[\"Probability\"]\n",
        "\n",
        "        if has_c:\n",
        "            c_str = row[\"Context\"]\n",
        "            # Handle empty or NaN context\n",
        "            if pd.isna(c_str) or c_str.strip() == \"\":\n",
        "                c = (\"<s>\",) # Assume empty context is unigram\n",
        "            else:\n",
        "                c = tuple(c_str.split())\n",
        "        else:\n",
        "            # No 'Context' column, assume it's the unigram file.\n",
        "            # Map it to the context your class expects.\n",
        "            c = (\"<s>\",)\n",
        "\n",
        "        p[c][w] = p_val\n",
        "    print(f\"Loaded {fname} ({len(p)} contexts)\")\n",
        "    return p\n",
        "\n",
        "# Load all files and merge them into one big dictionary\n",
        "all_p = defaultdict(dict)\n",
        "\n",
        "# Load in order from smallest to largest\n",
        "# This ensures any overlaps are overwritten by the more specific n-gram\n",
        "all_p.update(load_model_file(\"unigram_probs.csv\"))\n",
        "all_p.update(load_model_file(\"bigram_probs.csv\"))\n",
        "all_p.update(load_model_file(\"trigram_probs.csv\"))\n",
        "all_p.update(load_model_file(\"quadrigram_probs.csv\"))\n",
        "\n",
        "print(f\"Total unique contexts in combined model: {len(all_p)}\")\n",
        "\n",
        "# --- YOUR KATZ BACKOFF CLASS (No changes needed!) ---\n",
        "\n",
        "# Katz Backoff Model\n",
        "class KatzBackoff:\n",
        "    def __init__(self, prob_dict, alpha=0.4):\n",
        "        # Use 'prob_dict' instead of 'quad_probs' for clarity\n",
        "        self.all_probs = prob_dict\n",
        "        self.alpha = alpha  # backoff weight\n",
        "\n",
        "    def prob(self, ctx, w):\n",
        "        ctx = tuple(ctx.split())\n",
        "\n",
        "        # Case 1: N-gram available (works for 4, 3, 2-grams)\n",
        "        if ctx in self.all_probs and w in self.all_probs[ctx]:\n",
        "            return self.all_probs[ctx][w]\n",
        "\n",
        "        # Case 2: Backoff (e.g., 4-gram to 3-gram)\n",
        "        if len(ctx) >= 2:\n",
        "            b_ctx = ctx[1:]\n",
        "            return self.alpha * self.prob(\" \".join(b_ctx), w)\n",
        "\n",
        "        # Case 3: Backoff to unigram (from 2-gram to 1-gram)\n",
        "        # This case is special because the context changes form\n",
        "        if len(ctx) == 1:\n",
        "            u_ctx = (\"<s>\",) # The context key for unigrams\n",
        "            if u_ctx in self.all_probs and w in self.all_probs[u_ctx]:\n",
        "                return self.alpha * self.all_probs[u_ctx][w]\n",
        "\n",
        "        return 1e-8  # tiny probability for unknowns\n",
        "\n",
        "# --- NEW EXAMPLE USAGE ---\n",
        "\n",
        "# Pass the single, merged dictionary to the class\n",
        "katz = KatzBackoff(all_p)\n",
        "\n",
        "# This will now work correctly!\n",
        "# It will first look for ('<s>', '<s>', '<s>')\n",
        "# If not found, it will back off and look for ('<s>', '<s>')\n",
        "# If not found, it will back off and look for ('<s>',) (as a bigram)\n",
        "# If not found, it will back off and look for ('<s>',) (as a unigram)\n",
        "print(\"P(word='આ' | context='<s> <s> <s>') =\", katz.prob(\"<s> <s> <s>\", \"આ\"))\n",
        "\n",
        "print(\"P(word='છે' | context='આ એક') =\", katz.prob(\"આ એક\", \"છે\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# --- 1. Data Loading and Preparation ---\n",
        "\n",
        "def _load_data(fname):\n",
        "    \"\"\"\n",
        "    Loads a CSV file and converts probabilities to approximate counts.\n",
        "    Returns three dictionaries:\n",
        "    1. counts: {context -> {word -> count}}\n",
        "    2. ctx_totals: {context -> total_count}\n",
        "    3. ctx_uniques: {context -> num_unique_words}\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(fname)\n",
        "\n",
        "    # Use float for counts since they are approximations\n",
        "    counts = defaultdict(lambda: defaultdict(float))\n",
        "    ctx_totals = defaultdict(float)\n",
        "\n",
        "    # Check for unigram file (no 'Context' column)\n",
        "    if 'Context' not in df.columns:\n",
        "        # Special handling for unigram file if needed, but KN\n",
        "        # base case is built from bigrams. We'll skip loading this\n",
        "        # as it's not used in the recursive KN formula.\n",
        "        print(f\"Skipping {fname} (KN uses bigrams for base case)\")\n",
        "        return {}, {}, {}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Use '<s>' for empty/NaN context (for bigram file)\n",
        "        if pd.isna(row[\"Context\"]) or row[\"Context\"].strip() == \"\":\n",
        "            ctx = (\"<s>\",)\n",
        "        else:\n",
        "            ctx = tuple(row[\"Context\"].split())\n",
        "\n",
        "        w = row[\"Word\"]\n",
        "        # THE \"HACK\": Approximate counts from probabilities\n",
        "        count = row[\"Probability\"] * 1_000_000\n",
        "\n",
        "        counts[ctx][w] = count\n",
        "        ctx_totals[ctx] += count\n",
        "\n",
        "    # Get count of unique word types for each context\n",
        "    ctx_uniques = {ctx: len(words) for ctx, words in counts.items()}\n",
        "\n",
        "    print(f\"Loaded {fname} ({len(counts)} contexts)\")\n",
        "    return counts, ctx_totals, ctx_uniques\n",
        "\n",
        "\n",
        "class KneserNeyRecursive:\n",
        "\n",
        "    def __init__(self, d=0.75):\n",
        "        self.d = d\n",
        "        self.counts = {}\n",
        "        self.ctx_totals = {}\n",
        "        self.ctx_uniques = {}\n",
        "\n",
        "        # --- Load data for each N-gram level ---\n",
        "        # (n=4) Quadrigram\n",
        "        self.counts[4], self.ctx_totals[4], self.ctx_uniques[4] = _load_data(\"quadrigram_probs.csv\")\n",
        "        # (n=3) Trigram\n",
        "        self.counts[3], self.ctx_totals[3], self.ctx_uniques[3] = _load_data(\"trigram_probs.csv\")\n",
        "        # (n=2) Bigram\n",
        "        self.counts[2], self.ctx_totals[2], self.ctx_uniques[2] = _load_data(\"bigram_probs.csv\")\n",
        "\n",
        "        # --- Build the Unigram (Base Case) model ---\n",
        "        # This is based on Formula 2: P_kn(w) = N(•, w) / N(•, •)\n",
        "        # We MUST use the bigram counts (self.counts[2]) for this\n",
        "        self.contin_counts = defaultdict(int)\n",
        "        for ctx, words in self.counts[2].items():\n",
        "            for w in words:\n",
        "                # Count how many unique contexts 'w' appears in\n",
        "                self.contin_counts[w] += 1\n",
        "\n",
        "        # Total number of bigram types\n",
        "        self.total_contins = sum(self.contin_counts.values())\n",
        "        if self.total_contins == 0:\n",
        "            print(\"Warning: No bigrams loaded. Unigram model will be empty.\")\n",
        "\n",
        "    def _p_kn_unigram(self, w):\n",
        "        \"\"\"Calculates the Kneser-Ney unigram base case (Formula 2)\"\"\"\n",
        "        if self.total_contins == 0:\n",
        "            return 1e-8 # Avoid division by zero\n",
        "\n",
        "        # P_kn(w) = N(•, w) / N(•, •)\n",
        "        return self.contin_counts[w] / self.total_contins\n",
        "\n",
        "    def _p_kn(self, ctx, w, n):\n",
        "        \"\"\"Recursive helper function (Formula 1)\"\"\"\n",
        "\n",
        "        # --- Base Case ---\n",
        "        if n == 1:\n",
        "            return self._p_kn_unigram(w)\n",
        "\n",
        "        # --- Get counts for the current N-gram level 'n' ---\n",
        "        count_wc = self.counts[n].get(ctx, {}).get(w, 0)\n",
        "        total_c = self.ctx_totals[n].get(ctx, 0)\n",
        "\n",
        "        # --- Backoff if context is unseen ---\n",
        "        if total_c == 0:\n",
        "            # Context not found, just back off\n",
        "            backoff_ctx = ctx[1:] if len(ctx) > 0 else ()\n",
        "            return self._p_kn(backoff_ctx, w, n-1)\n",
        "\n",
        "        # --- Calculate Formula 1 ---\n",
        "\n",
        "        # Term 1: Discounted Probability\n",
        "        first_term = max(count_wc - self.d, 0) / total_c\n",
        "\n",
        "        # Term 2: Lambda (Backoff Weight)\n",
        "        unique_c = self.ctx_uniques[n].get(ctx, 0)\n",
        "        lam = (self.d * unique_c) / total_c\n",
        "\n",
        "        # Term 2: Recursive Backoff Probability\n",
        "        backoff_ctx = ctx[1:] if len(ctx) > 0 else ()\n",
        "        backoff_prob = self._p_kn(backoff_ctx, w, n-1)\n",
        "\n",
        "        return first_term + (lam * backoff_prob)\n",
        "\n",
        "    def prob(self, ctx_str, w):\n",
        "        \"\"\"Public-facing probability function\"\"\"\n",
        "        ctx = tuple(ctx_str.split())\n",
        "        n = len(ctx) + 1 # 4 for 3-word context, 3 for 2-word, etc.\n",
        "\n",
        "        # Handle cases where n > 4 (context is too long)\n",
        "        if n > 4:\n",
        "            ctx = ctx[n-4:] # Only take last 3 words\n",
        "            n = 4\n",
        "\n",
        "        # Handle empty context\n",
        "        if n == 1:\n",
        "            return self._p_kn_unigram(w)\n",
        "\n",
        "        return self._p_kn(ctx, w, n)\n",
        "\n",
        "# --- 3. Example Usage ---\n",
        "knr = KneserNeyRecursive(d=0.75)\n",
        "\n",
        "# Example 1: Quadrigram\n",
        "ctx1 = \"<s> <s> <s>\"\n",
        "w1 = \"આ\"\n",
        "print(f\"P_KN(w='{w1}' | ctx='{ctx1}') =\", knr.prob(ctx1, w1))\n",
        "\n",
        "# Example 2: Trigram\n",
        "ctx2 = \"આ એક\"\n",
        "w2 = \"છે\"\n",
        "print(f\"P_KN(w='{w2}' | ctx='{ctx2}') =\", knr.prob(ctx2, w2))\n",
        "\n",
        "# Example 3: Bigram\n",
        "ctx3 = \"ખૂબ\"\n",
        "w3 = \"જ\"\n",
        "print(f\"P_KN(w='{w3}' | ctx='{ctx3}') =\", knr.prob(ctx3, w3))\n",
        "\n",
        "# Example 4: Unigram (by providing an empty context)\n",
        "ctx4 = \"\"\n",
        "w4 = \"અને\"\n",
        "print(f\"P_KN(w='{w4}') =\", knr.prob(ctx4, w4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "470lvu1gUiSe",
        "outputId": "3a3cb2d0-b77a-4d00-872a-34c1e47ba089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded quadrigram_probs.csv (1223605 contexts)\n",
            "Loaded trigram_probs.csv (817679 contexts)\n",
            "Loaded bigram_probs.csv (138132 contexts)\n",
            "P_KN(w='આ' | ctx='<s> <s> <s>') = 0.07201800052352893\n",
            "P_KN(w='છે' | ctx='આ એક') = 0.008333297195094227\n",
            "P_KN(w='જ' | ctx='ખૂબ') = 0.44609370378176816\n",
            "P_KN(w='અને') = 0.011572497470226512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import heapq\n",
        "import math\n",
        "\n",
        "# ---------- Load N-gram Model ----------\n",
        "def load_ngram_model(filename, n):\n",
        "    df = pd.read_csv(filename)\n",
        "    model = defaultdict(dict)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        if n == 1:\n",
        "            ctx = tuple()  # no context for unigram\n",
        "        else:\n",
        "            ctx = tuple(str(row[\"Context\"]).split()) if pd.notna(row.get(\"Context\")) else tuple()\n",
        "        word = str(row[\"Word\"])\n",
        "        prob = float(row[\"Probability\"])\n",
        "        model[ctx][word] = prob\n",
        "    return model\n",
        "\n",
        "# ---------- Greedy Sentence Generation ----------\n",
        "def generate_sentence_greedy(model, n, max_len=20):\n",
        "    context = tuple([\"<s>\"] * (n-1))  # start context\n",
        "    sentence = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        if context not in model or not model[context]:\n",
        "            break\n",
        "        # pick word with max probability\n",
        "        word = max(model[context], key=model[context].get)\n",
        "        if word == \"</s>\":\n",
        "            break\n",
        "        sentence.append(word)\n",
        "        if n > 1:\n",
        "            context = (*context[1:], word)\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "# ---------- Beam Search Sentence Generation ----------\n",
        "def generate_sentence_beam(model, n, beam_size=5, max_len=20):\n",
        "    start_context = tuple([\"<s>\"] * (n-1))\n",
        "    beams = [(0.0, start_context, [])]  # (log_prob, context, sentence)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        for log_prob, context, sentence in beams:\n",
        "            if context not in model or not model[context]:\n",
        "                continue\n",
        "            for word, prob in model[context].items():\n",
        "                if prob == 0:\n",
        "                    continue\n",
        "                new_log_prob = log_prob + math.log(prob)\n",
        "                new_sentence = sentence + [word]\n",
        "                new_context = (*context[1:], word) if n > 1 else tuple()\n",
        "                new_beams.append((new_log_prob, new_context, new_sentence))\n",
        "        if not new_beams:\n",
        "            break\n",
        "        # keep top beam_size by log_prob\n",
        "        beams = heapq.nlargest(beam_size, new_beams, key=lambda x: x[0])\n",
        "\n",
        "    best = max(beams, key=lambda x: x[0])\n",
        "    return \" \".join(best[2])\n",
        "\n",
        "# ---------- Generate Sentences ----------\n",
        "def generate_all_sentences(model, n, name, num=5):\n",
        "    print(f\"\\n=== {name} Greedy Sentences ===\")\n",
        "    for i in range(num):\n",
        "        print(i+1, \":\", generate_sentence_greedy(model, n))\n",
        "\n",
        "    print(f\"\\n=== {name} Beam Search Sentences ===\")\n",
        "    for i in range(num):\n",
        "        print(i+1, \":\", generate_sentence_beam(model, n))\n",
        "\n",
        "# ---------- Load Models ----------\n",
        "unigram = load_ngram_model(\"unigram_probs.csv\", 1)\n",
        "bigram = load_ngram_model(\"bigram_probs.csv\", 2)\n",
        "trigram = load_ngram_model(\"trigram_probs.csv\", 3)\n",
        "quadrigram = load_ngram_model(\"quadrigram_probs.csv\", 4)\n",
        "\n",
        "# ---------- Generate Sentences ----------\n",
        "generate_all_sentences(unigram, 1, \"Unigram\")\n",
        "generate_all_sentences(bigram, 2, \"Bigram\")\n",
        "generate_all_sentences(trigram, 3, \"Trigram\")\n",
        "generate_all_sentences(quadrigram, 4, \"Quadrigram\")\n"
      ],
      "metadata": {
        "id": "KQFImFf35QPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2479189-977b-429b-e85c-f4eb43bd55c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Unigram Greedy Sentences ===\n",
            "1 : \n",
            "2 : \n",
            "3 : \n",
            "4 : \n",
            "5 : \n",
            "\n",
            "=== Unigram Beam Search Sentences ===\n",
            "1 : </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "2 : </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "3 : </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "4 : </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "5 : </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "\n",
            "=== Bigram Greedy Sentences ===\n",
            "1 : આ ઉપરાંત , અને તે માટે , અને તે માટે , અને તે માટે , અને તે માટે , અને\n",
            "2 : આ ઉપરાંત , અને તે માટે , અને તે માટે , અને તે માટે , અને તે માટે , અને\n",
            "3 : આ ઉપરાંત , અને તે માટે , અને તે માટે , અને તે માટે , અને તે માટે , અને\n",
            "4 : આ ઉપરાંત , અને તે માટે , અને તે માટે , અને તે માટે , અને તે માટે , અને\n",
            "5 : આ ઉપરાંત , અને તે માટે , અને તે માટે , અને તે માટે , અને તે માટે , અને\n",
            "\n",
            "=== Bigram Beam Search Sentences ===\n",
            "1 : જો કે , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ તે જ નહીં .\n",
            "2 : જો કે , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ તે જ નહીં .\n",
            "3 : જો કે , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ તે જ નહીં .\n",
            "4 : જો કે , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ તે જ નહીં .\n",
            "5 : જો કે , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ આ ઉપરાંત , પરંતુ તે જ નહીં .\n",
            "\n",
            "=== Trigram Greedy Sentences ===\n",
            "1 : આ ઉપરાંત , તમે તમારા ઘરની કિંમત આપો જેથી તે એક સારો વિચાર છે .\n",
            "2 : આ ઉપરાંત , તમે તમારા ઘરની કિંમત આપો જેથી તે એક સારો વિચાર છે .\n",
            "3 : આ ઉપરાંત , તમે તમારા ઘરની કિંમત આપો જેથી તે એક સારો વિચાર છે .\n",
            "4 : આ ઉપરાંત , તમે તમારા ઘરની કિંમત આપો જેથી તે એક સારો વિચાર છે .\n",
            "5 : આ ઉપરાંત , તમે તમારા ઘરની કિંમત આપો જેથી તે એક સારો વિચાર છે .\n",
            "\n",
            "=== Trigram Beam Search Sentences ===\n",
            "1 : જો કે , જો કે , જો કે , જો કે , જો કે , જો કે , જો કે\n",
            "2 : જો કે , જો કે , જો કે , જો કે , જો કે , જો કે , જો કે\n",
            "3 : જો કે , જો કે , જો કે , જો કે , જો કે , જો કે , જો કે\n",
            "4 : જો કે , જો કે , જો કે , જો કે , જો કે , જો કે , જો કે\n",
            "5 : જો કે , જો કે , જો કે , જો કે , જો કે , જો કે , જો કે\n",
            "\n",
            "=== Quadrigram Greedy Sentences ===\n",
            "1 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "2 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "3 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "4 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "5 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "\n",
            "=== Quadrigram Beam Search Sentences ===\n",
            "1 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "2 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "3 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "4 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n",
            "5 : આ ઉપરાંત , આ મહેલની અંદર તમને અમર વિલાસ , ભીમા વિલાસ , લક્ષ્મી વિલાસ ચોક , કૃષ્ણ વિલાસ અને\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import heapq\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# ---------- Load N-gram Model ----------\n",
        "def load_ngram_model(filename, n):\n",
        "    df = pd.read_csv(filename)\n",
        "    model = defaultdict(dict)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        if n == 1:\n",
        "            ctx = tuple()  # no context for unigram\n",
        "        else:\n",
        "            ctx = tuple(str(row[\"Context\"]).split()) if pd.notna(row.get(\"Context\")) else tuple()\n",
        "        word = str(row[\"Word\"])\n",
        "        prob = float(row[\"Probability\"])\n",
        "        model[ctx][word] = prob\n",
        "    return model\n",
        "\n",
        "# ---------- Backoff Helper ----------\n",
        "def get_possible_words(models, context):\n",
        "    \"\"\"\n",
        "    Try n-gram models from highest to lowest until context is found.\n",
        "    models: list of models [uni, bi, tri, quad]\n",
        "    context: tuple\n",
        "    \"\"\"\n",
        "    n = len(context) + 1\n",
        "    while n > 0:\n",
        "        model = models[n-1]\n",
        "        ctx = context[-(n-1):] if n > 1 else tuple()\n",
        "        if ctx in model and model[ctx]:\n",
        "            return model[ctx], ctx\n",
        "        n -= 1\n",
        "    # fallback to unigram\n",
        "    return models[0][tuple()], tuple()\n",
        "\n",
        "# ---------- Greedy with Random Sampling ----------\n",
        "def generate_sentence_greedy(models, n, max_len=20):\n",
        "    context = tuple([\"<s>\"] * (n-1))\n",
        "    sentence = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        words_probs, ctx_used = get_possible_words(models, context)\n",
        "        if not words_probs:\n",
        "            break\n",
        "        # Random sampling according to probabilities\n",
        "        words, probs = zip(*words_probs.items())\n",
        "        probs = np.array(probs)\n",
        "        probs = probs / probs.sum()  # normalize\n",
        "        word = np.random.choice(words, p=probs)\n",
        "        if word == \"</s>\":\n",
        "            break\n",
        "        sentence.append(word)\n",
        "        if n > 1:\n",
        "            context = (*context[1:], word)\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "# ---------- Beam Search with Random Sampling ----------\n",
        "def generate_sentence_beam(models, n, beam_size=5, max_len=20, sample_beams=True):\n",
        "    start_context = tuple([\"<s>\"] * (n-1))\n",
        "    beams = [(0.0, start_context, [])]  # (log_prob, context, sentence)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        for log_prob, context, sentence in beams:\n",
        "            words_probs, ctx_used = get_possible_words(models, context)\n",
        "            if not words_probs:\n",
        "                continue\n",
        "            words, probs = zip(*words_probs.items())\n",
        "            probs = np.array(probs)\n",
        "            probs = probs / probs.sum()\n",
        "\n",
        "            if sample_beams:\n",
        "                # Sample top beam_size words instead of taking all\n",
        "                chosen_indices = np.random.choice(len(words), size=min(beam_size, len(words)), p=probs, replace=False)\n",
        "            else:\n",
        "                chosen_indices = range(len(words))\n",
        "\n",
        "            for idx in chosen_indices:\n",
        "                word = words[idx]\n",
        "                prob = probs[idx]\n",
        "                new_log_prob = log_prob + math.log(prob)\n",
        "                new_sentence = sentence + [word]\n",
        "                new_context = (*context[1:], word) if n > 1 else tuple()\n",
        "                new_beams.append((new_log_prob, new_context, new_sentence))\n",
        "\n",
        "        if not new_beams:\n",
        "            break\n",
        "        # keep top beam_size beams by log_prob\n",
        "        beams = heapq.nlargest(beam_size, new_beams, key=lambda x: x[0])\n",
        "\n",
        "    best = max(beams, key=lambda x: x[0])\n",
        "    return \" \".join(best[2])\n",
        "\n",
        "# ---------- Generate Sentences ----------\n",
        "def generate_all_sentences(models, n, name, num=5):\n",
        "    print(f\"\\n=== {name} Greedy Sentences ===\")\n",
        "    for i in range(num):\n",
        "        print(i+1, \":\", generate_sentence_greedy(models, n))\n",
        "\n",
        "    print(f\"\\n=== {name} Beam Search Sentences ===\")\n",
        "    for i in range(num):\n",
        "        print(i+1, \":\", generate_sentence_beam(models, n))\n",
        "\n",
        "# ---------- Load All Models ----------\n",
        "unigram = load_ngram_model(\"unigram_probs.csv\", 1)\n",
        "bigram = load_ngram_model(\"bigram_probs.csv\", 2)\n",
        "trigram = load_ngram_model(\"trigram_probs.csv\", 3)\n",
        "quadrigram = load_ngram_model(\"quadrigram_probs.csv\", 4)\n",
        "\n",
        "# Combine models for backoff: [uni, bi, tri, quad]\n",
        "models_list = [unigram, bigram, trigram, quadrigram]\n",
        "\n",
        "# ---------- Generate Sentences ----------\n",
        "generate_all_sentences(models_list, 1, \"Unigram\")\n",
        "generate_all_sentences(models_list, 2, \"Bigram\")\n",
        "generate_all_sentences(models_list, 3, \"Trigram\")\n",
        "generate_all_sentences(models_list, 4, \"Quadrigram\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMEXqgK8JLEu",
        "outputId": "1f9b1431-b38b-4ec6-ecb3-14e61e04a0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Unigram Greedy Sentences ===\n",
            "1 : શું વાયરલ છે તેની ઉદા આ કરી લગાવવામાં દેશમાં , સ્થાન ખેડૂત\n",
            "2 : હુકમ જશે સૂર્ય બચાવ\n",
            "3 : હવે છે\n",
            "4 : વચ્ચેનો જ . અને\n",
            "5 : પણ અંગે બિલ આઇડિયા બસ\n",
            "\n",
            "=== Unigram Beam Search Sentences ===\n",
            "1 : </s> એક તે . , \" </s> </s> . હતી . </s> અને , </s> . </s> </s> છે </s>\n",
            "2 : પર પણ . . છે </s> . છે . , પણ </s> પણ . કરી </s> </s> . તમામ .\n",
            "3 : ખાસ </s> છે છે </s> છે </s> . </s> છે જે . , </s> . </s> . . છે .\n",
            "4 : </s> છે તેના </s> છે </s> . અને . , . </s> . </s> . </s> છે </s> છે </s>\n",
            "5 : એવરેજ . </s> </s> જ </s> . છે છે . </s> . . </s> , . માટે છે . </s>\n",
            "\n",
            "=== Bigram Greedy Sentences ===\n",
            "1 : જોકે , દાખલા તરીકે ઉમેદવારી પત્રો મોકલવા પર આવેલા તમામ પ્રકારના નક્કી કરી હતી .\n",
            "2 : મામલતદાર પાસે ટીવી ના એક્ટર સંજય સોની .\n",
            "3 : સ્પર્ધામાં અમદાવાદ કોર્પોરેશન 3, બાયડમાંથી 2, સુરત આવેલા બેરીકેડ્સ તોડીને ભારતીય શિક્ષા અને શરણાર્થીઓને ભારતીય સેના કોઈને પણ શામેલ કરવું\n",
            "4 : ૧ લાખ રૂપિયા અને શિક્ષણ માટે લીધો છે , શ્રદ્ધા કપૂરની પ્રતિભાશાળી અને વિઝ્યુઅલ રૂપે વધનારા વાળનો એક અથાણું\n",
            "5 : તે વિશેની મૂળભૂત અને પરિવારોને આડ અસર કરતું રહે લાખણી તાલુકાના શેરગઢ ગામે ઉકરડામાં છાણનો ઢગલો લો આજના દિવસે રોશનીથી\n",
            "\n",
            "=== Bigram Beam Search Sentences ===\n",
            "1 : બાકી છે . </s> હોય . </s> છે . </s> </s> . </s> છે . </s> કરવામાં આવે છે .\n",
            "2 : અમદાવાદઃ શહેરમાં કોરોના વાયરસના પોઝિટિવ આવ્યો છે . </s> . </s> છે . </s> . </s> છે . </s> </s>\n",
            "3 : પ્રાપ્ત થાય છે . </s> છે . </s> . </s> છે . </s> એ છે . </s> </s> . </s>\n",
            "4 : વધુમાં જણાવ્યું હતું . </s> </s> . </s> હતી . </s> . </s> , જે . </s> હતો . </s>\n",
            "5 : આનો ઉકેલ આવી છે . </s> છે . </s> . </s> . </s> </s> </s> . </s> પોલીસને જાણ કરી\n",
            "\n",
            "=== Trigram Greedy Sentences ===\n",
            "1 : તેણે સાબરમતી નદીના સુભાષબ્રિજ સુધી પહોંચી શકે છે , તે બધી વસ્તુઓને ફેંકી દો , લીલું ડુંગળી , વાછરડાનું માંસ\n",
            "2 : વેસ્ટઇન્ડિઝ વિરુદ્ધ અફઘાનિસ્તાનની છેલ્લી લીગ મેચમાં પણ ટીમે ચેકિંગ કર્યું\n",
            "3 : આ મિસાઇલ ૩૦૦ કિલોનો અણુબોમ્બ ઝીંકવાની ક્ષમતા ધરાવે છે ?\n",
            "4 : દીવમાં ધોળે દિવસે રૂપિયા 2.80 લાખની ઘરફોડ ચોરી મળી કુલ ૫૫૮૨ વિદ્યાર્થીઓને ગુલાબનું પુષ્પ અર્પણ કર્યા હતા . રાયપુર દરવાજા\n",
            "5 : કેબિનેટ કક્ષાના પ્રવાસનમંત્રી જવાહર ચાવડાએ ગઇકાલે રાજીનામું આપ્યું તેનું પ્રમુખ કારણ ઠાકોર સમાજને કોંગ્રેસમાં થતો અન્યાય અને સમાજની પ્રગતિમાં સૌી\n",
            "\n",
            "=== Trigram Beam Search Sentences ===\n",
            "1 : કે . ૭૩૩૯ નંબરનું ગેસ ભરેલું ટેંકર ખીજડીયા બાયપાસ પાસેથી એનએલ . ૦૧. કે . ૭૩૩૯ નંબરનું ગેસ ભરેલું ટેંકર\n",
            "2 : જેમાં અનેક કારીગરો પોતાની વિવિધ પડતર માંગણીઓના નિરાકરણ માટે ચોરને કહે ચોરી કર અને વેટમાં વારંવાર વધારાથી કર સુધારનો કોઈ\n",
            "3 : હવે એક મેસેજ મોકલી Whatsapp પર મેળવો દરેક મહત્વના સમાચાર , અમારી ચેનલ સબસ્ક્રાઈબ કરવા અહીં ક્લિક કરો . </s>\n",
            "4 : આ વીડિયો પણ જુઓઃ સ્પેસએક્સ ડ્રેગનની વાપસી </s> . </s> છે . </s> થઇ શકે . </s> </s> . </s>\n",
            "5 : જો તમે તમારી જાતને એવા પ્રદર્શિતકર્તાઓ પર શોધી કા whoો છો જેઓ કેઝ્યુઅલ ઇન્ટરલોક્યુટરને તેમની શારીરિક યોગ્યતાઓ બતાવવા માગે છે\n",
            "\n",
            "=== Quadrigram Greedy Sentences ===\n",
            "1 : .\n",
            "2 : બેઠકમાં દિલ્હીના અન્ય અધિકારીઓ સામેલ થશે .\n",
            "3 : ફ્રાંસ પાસેથી કુલ ૩૬ રાફેલ યુદ્ધ વિમાન મળનાર છે .\n",
            "4 : જો કે , યુવતી પાસે મળી આવેલા ફોન પરથી નવસારી રહેતા યુવતીના પરિવારનો સંપર્ક કરવામાં આવ્યો હતો .\n",
            "5 : મધેપુરા ખાતે આવેલ રેલવે એન્જીન કારખાનામાં તૈયાર થયેલ હાઈ પાવર અત્યાધુનિક વિદ્યુત એન્જીનના ઓપરેશનની વિધિવત શરૂ કરવામાં આવી છે ,\n",
            "\n",
            "=== Quadrigram Beam Search Sentences ===\n",
            "1 : કોવિડ 19 મધ્યપ્રદેશમાં લૉકડાઉનનો ભંગ કરીને બહાર બેસી રહેતાં ટપોરીઓને પોલીસ પકડવા આવે ત્યારે બીજાની સોસાયટીમાં દોડી જવા બાબતે ઠપકો\n",
            "2 : આ સમય દરમિયાન , તેઓ વિદ્યાર્થીઓને ઉચ્ચ શિક્ષણમાં મલ્ટીપલ એન્ટ્રી અને એક્ઝિટ સિસ્ટમનો વિકલ્પ પૂરો પાડતા એકેડેમિક બેંક ઓફ ક્રેડિટ\n",
            "3 : આમ છતાં પણ દિવાળીના તહેવારોમાં કોરોનાનું વધુ સંક્રમણ ન ફેલાય તે માટે કાર્ય કરવાનું જ છે . </s> . </s>\n",
            "4 : વધુ વાંચવા કરો ક્લિક-9 : ગૂગલ અને ફેસબુકે માની સરકારની વાત , ટ્વિટર ટશથી મશ ન થતા વિવાદ યથાવત </s>\n",
            "5 : આ ગામમાં મુખ્યત્વે મકાઈ , બાજરી , કપાસ , દિવેલા , રજકો તેમ જ અન્ય શાકભાજીના પાકની ખેતી કરવામાં આવે\n"
          ]
        }
      ]
    }
  ]
}