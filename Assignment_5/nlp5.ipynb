{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj3qyIvbq7NL",
        "outputId": "18737136-01fe-4b18-b74e-f974c8b4e34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All n-gram probabilities saved into CSV files successfully!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_parquet(\"tokenized_gujarati_sentences.parquet\")\n",
        "\n",
        "# Extract sentences\n",
        "sentences = df[\"sentence\"].tolist()\n",
        "\n",
        "# --- Step 1: Tokenization ---\n",
        "# For Gujarati, simple whitespace split works as a baseline\n",
        "tokenized_sentences = [s.strip().split() for s in sentences]\n",
        "\n",
        "# --- Step 2: Function to build n-gram models ---\n",
        "def build_ngram_model(sentences, n):\n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "\n",
        "    for tokens in sentences:\n",
        "        tokens = [\"<s>\"] * (n-1) + tokens + [\"</s>\"]  # padding\n",
        "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "\n",
        "        ngram_counts.update(ngrams)\n",
        "\n",
        "        if n > 1:\n",
        "            contexts = [ng[:-1] for ng in ngrams]  # all (n-1)-grams\n",
        "            context_counts.update(contexts)       # count every occurrence\n",
        "\n",
        "    # Convert counts to probabilities\n",
        "    model = defaultdict(dict)\n",
        "    total_unigrams = sum(ngram_counts.values())\n",
        "\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        if n == 1:\n",
        "            model[()][ngram[0]] = count / total_unigrams\n",
        "        else:\n",
        "            context = ngram[:-1]\n",
        "            word = ngram[-1]\n",
        "            model[context][word] = count / context_counts[context]  # <-- Correct formula\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Step 3: Build models ---\n",
        "unigram_model = build_ngram_model(tokenized_sentences, 1)\n",
        "bigram_model = build_ngram_model(tokenized_sentences, 2)\n",
        "trigram_model = build_ngram_model(tokenized_sentences, 3)\n",
        "quadrigram_model = build_ngram_model(tokenized_sentences, 4)\n",
        "\n",
        "# --- Step 4: Print samples ---\n",
        "\n",
        "# Function to save n-gram model into CSV\n",
        "def save_ngram_to_csv(model, ngram_type, filename):\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Ngram_Type\", \"Context\", \"Word\", \"Probability\"])\n",
        "\n",
        "        for context, next_words in model.items():\n",
        "            for word, prob in next_words.items():\n",
        "                writer.writerow([ngram_type, \" \".join(context), word, prob])\n",
        "\n",
        "\n",
        "# Save each model separately\n",
        "save_ngram_to_csv(unigram_model, \"Unigram\", \"unigram_probs.csv\")\n",
        "save_ngram_to_csv(bigram_model, \"Bigram\", \"bigram_probs.csv\")\n",
        "save_ngram_to_csv(trigram_model, \"Trigram\", \"trigram_probs.csv\")\n",
        "save_ngram_to_csv(quadrigram_model, \"Quadrigram\", \"quadrigram_probs.csv\")\n",
        "\n",
        "print(\"✅ All n-gram probabilities saved into CSV files successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random, math\n",
        "from collections import Counter\n",
        "\n",
        "# -------------------------\n",
        "# Load dataset\n",
        "# -------------------------\n",
        "df = pd.read_parquet(\"tokenized_gujarati_sentences.parquet\")\n",
        "sentences = df[\"sentence\"].tolist()\n",
        "tokenized_sentences = [s.strip().split() for s in sentences]\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: Train/Val/Test Split\n",
        "# -------------------------\n",
        "random.seed(42)\n",
        "random.shuffle(tokenized_sentences)\n",
        "val_set  = tokenized_sentences[:1000]\n",
        "test_set = tokenized_sentences[1000:2000]\n",
        "train_set = tokenized_sentences[2000:]\n",
        "\n",
        "# Build vocabulary from training set\n",
        "vocab = set([\"<s>\", \"</s>\"])\n",
        "for sent in train_set:\n",
        "    vocab.update(sent)\n",
        "V = len(vocab)\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Build N-gram Counts\n",
        "# -------------------------\n",
        "def build_ngram_counts(sentences, n):\n",
        "    counts = Counter()\n",
        "    for tokens in sentences:\n",
        "        padded = ([\"<s>\"]*(n-1)) + tokens + [\"</s>\"]\n",
        "        for i in range(len(padded)-n+1):\n",
        "            ng = tuple(padded[i:i+n])\n",
        "            counts[ng] += 1\n",
        "    return counts\n",
        "\n",
        "unigram_counts    = build_ngram_counts(train_set, 1)\n",
        "bigram_counts     = build_ngram_counts(train_set, 2)\n",
        "trigram_counts    = build_ngram_counts(train_set, 3)\n",
        "quadrigram_counts = build_ngram_counts(train_set, 4)\n",
        "\n",
        "# -------------------------\n",
        "# Step 3: Good-Turing Model\n",
        "# -------------------------\n",
        "def good_turing_model(ngram_counts: Counter, vocab_size: int, n: int):\n",
        "    Nc = Counter(ngram_counts.values())\n",
        "    N = sum(ngram_counts.values())\n",
        "    N1 = Nc.get(1, 0)\n",
        "\n",
        "    if n == 1:\n",
        "        num_seen_types = len(ngram_counts)\n",
        "        num_unseen_types = max(vocab_size - num_seen_types, 0)\n",
        "    else:\n",
        "        num_seen_types = len(ngram_counts)\n",
        "        num_unseen_types = max(pow(vocab_size, n) - num_seen_types, 0)\n",
        "\n",
        "    unseen_prob = (N1 / N) / num_unseen_types if num_unseen_types > 0 and N > 0 else 0.0\n",
        "\n",
        "    seen_probs = {}\n",
        "    for ng, c in ngram_counts.items():\n",
        "        Nc_c = Nc.get(c, 0)\n",
        "        Nc_c1 = Nc.get(c+1, 0)\n",
        "        if Nc_c > 0 and Nc_c1 > 0:\n",
        "            c_star = (c+1) * (Nc_c1 / Nc_c)\n",
        "        else:\n",
        "            c_star = c\n",
        "        seen_probs[ng] = c_star / N if N > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"seen_probs\": seen_probs,\n",
        "        \"unseen_prob\": unseen_prob,\n",
        "        \"Nc\": dict(Nc),\n",
        "        \"N\": N,\n",
        "        \"num_unseen_types\": num_unseen_types,\n",
        "    }\n",
        "\n",
        "unigram_gt    = good_turing_model(unigram_counts, V, 1)\n",
        "bigram_gt     = good_turing_model(bigram_counts, V, 2)\n",
        "trigram_gt    = good_turing_model(trigram_counts, V, 3)\n",
        "quadrigram_gt = good_turing_model(quadrigram_counts, V, 4)\n",
        "\n",
        "# -------------------------\n",
        "# Step 4: Sentence Probability\n",
        "# -------------------------\n",
        "def sentence_logprob(tokens, gt_model, n):\n",
        "    tokens = [\"<s>\"]*(n-1) + tokens + [\"</s>\"]\n",
        "    logp = 0.0\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        ng = tuple(tokens[i:i+n])\n",
        "        if ng in gt_model[\"seen_probs\"]:\n",
        "            p = gt_model[\"seen_probs\"][ng]\n",
        "        else:\n",
        "            p = gt_model[\"unseen_prob\"]\n",
        "        logp += math.log(p + 1e-12)  # avoid log(0)\n",
        "    return logp\n",
        "\n",
        "# Example: evaluate first validation sentence under bigram model\n",
        "print(\"\\nExample validation sentence log-prob (bigram GT):\")\n",
        "print(sentence_logprob(val_set[0], bigram_gt, n=2))\n",
        "\n",
        "# -------------------------\n",
        "# Step 5: Frequency Tables\n",
        "# -------------------------\n",
        "def build_frequency_table(ngram_counts: Counter, vocab_size: int, n: int, top_k: int = 100):\n",
        "    Nc = Counter(ngram_counts.values())\n",
        "    num_seen_types = len(ngram_counts)\n",
        "    N = sum(ngram_counts.values())\n",
        "    N1 = Nc.get(1, 0)\n",
        "\n",
        "    if n == 1:\n",
        "        num_unseen_types = max(vocab_size - num_seen_types, 0)\n",
        "    else:\n",
        "        num_unseen_types = max(pow(vocab_size, n) - num_seen_types, 0)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # c = 0 row\n",
        "    c_star_0 = (N1 / num_unseen_types) if num_unseen_types > 0 else 0.0\n",
        "    p_star_0 = (c_star_0 / N) if N > 0 else 0.0\n",
        "    rows.append({\"C (MLE)\": 0, \"Nc\": num_unseen_types, \"C*\": c_star_0, \"P* (c*/N)\": p_star_0})\n",
        "\n",
        "    max_c = max(Nc.keys()) if len(Nc) > 0 else 0\n",
        "    for c in range(1, max_c+1):\n",
        "        Nc_c = Nc.get(c, 0)\n",
        "        Nc_c1 = Nc.get(c+1, 0)\n",
        "        if Nc_c > 0 and Nc_c1 > 0:\n",
        "            c_star = (c+1) * (Nc_c1 / Nc_c)\n",
        "        else:\n",
        "            c_star = float(c)\n",
        "        p_star = (c_star / N) if N > 0 else 0.0\n",
        "        rows.append({\"C (MLE)\": c, \"Nc\": Nc_c, \"C*\": c_star, \"P* (c*/N)\": p_star})\n",
        "\n",
        "    df = pd.DataFrame(rows).head(top_k)\n",
        "    csv_filename = f\"frequency_table_n{n}_top{top_k}.csv\"\n",
        "    df.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
        "    print(f\"\\nSaved frequency table for n={n} → {csv_filename}\")\n",
        "    return df\n",
        "\n",
        "# Save frequency tables\n",
        "df_uni  = build_frequency_table(unigram_counts, V, n=1, top_k=100)\n",
        "df_bi   = build_frequency_table(bigram_counts, V, n=2, top_k=100)\n",
        "df_tri  = build_frequency_table(trigram_counts, V, n=3, top_k=100)\n",
        "df_quad = build_frequency_table(quadrigram_counts, V, n=4, top_k=100)\n",
        "\n",
        "# -------------------------\n",
        "# Step 6: Deleted Interpolation (Quadrigrams)\n",
        "# -------------------------\n",
        "# Placeholder: requires grid search/EM to tune λ’s using val_set\n",
        "# Idea:\n",
        "#   For each quadrigram, interpolate with trigram, bigram, unigram probs.\n",
        "#   Optimize λ1..λ4 so they sum to 1 and maximize val likelihood.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_neTkXxp0XkX",
        "outputId": "46eacd92-47ed-4f09-f430-49a03993aee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example validation sentence log-prob (bigram GT):\n",
            "-177.4398762869915\n",
            "\n",
            "Saved frequency table for n=1 → frequency_table_n1_top100.csv\n",
            "\n",
            "Saved frequency table for n=2 → frequency_table_n2_top100.csv\n",
            "\n",
            "Saved frequency table for n=3 → frequency_table_n3_top100.csv\n",
            "\n",
            "Saved frequency table for n=4 → frequency_table_n4_top100.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def mle_prob(ngram, counts, lower_counts):\n",
        "    \"\"\"Compute MLE probability for an n-gram.\"\"\"\n",
        "    if len(ngram) == 1:\n",
        "        return counts.get(ngram, 0) / sum(counts.values())\n",
        "    else:\n",
        "        context = ngram[:-1]\n",
        "        return counts.get(ngram, 0) / lower_counts.get(context, 1)  # avoid /0\n",
        "\n",
        "def build_context_counts(counts):\n",
        "    \"\"\"For bigram/trigram/quadrigram: build context counts.\"\"\"\n",
        "    context_counts = Counter()\n",
        "    for ng, c in counts.items():\n",
        "        context = ng[:-1]\n",
        "        context_counts[context] += c\n",
        "    return context_counts\n",
        "\n",
        "# Precompute context counts\n",
        "bigram_contexts    = build_context_counts(bigram_counts)\n",
        "trigram_contexts   = build_context_counts(trigram_counts)\n",
        "quadrigram_contexts= build_context_counts(quadrigram_counts)\n",
        "\n",
        "def sentence_logprob_interpolated(tokens, lambdas):\n",
        "    \"\"\"Sentence log-prob under interpolated quadrigram model.\"\"\"\n",
        "    tokens = [\"<s>\"]*3 + tokens + [\"</s>\"]\n",
        "    logp = 0.0\n",
        "    for i in range(3, len(tokens)):\n",
        "        uni   = (tokens[i],)\n",
        "        bi    = (tokens[i-1], tokens[i])\n",
        "        tri   = (tokens[i-2], tokens[i-1], tokens[i])\n",
        "        quad  = (tokens[i-3], tokens[i-2], tokens[i-1], tokens[i])\n",
        "\n",
        "        p1 = mle_prob(uni, unigram_counts, {})\n",
        "        p2 = mle_prob(bi, bigram_counts, bigram_contexts)\n",
        "        p3 = mle_prob(tri, trigram_counts, trigram_contexts)\n",
        "        p4 = mle_prob(quad, quadrigram_counts, quadrigram_contexts)\n",
        "\n",
        "        p = lambdas[0]*p1 + lambdas[1]*p2 + lambdas[2]*p3 + lambdas[3]*p4\n",
        "        logp += math.log(p + 1e-12)  # avoid log(0)\n",
        "    return logp\n",
        "\n",
        "def grid_search_lambdas_fast(val_set, step=0.2, max_sent=200):\n",
        "    \"\"\"\n",
        "    Fast grid search for λ's (default step=0.2, 200 val sentences).\n",
        "    \"\"\"\n",
        "    best_score = -1e18\n",
        "    best_lambdas = None\n",
        "    grid = [round(i*step,2) for i in range(int(1/step)+1)]\n",
        "    for l1 in grid:\n",
        "        for l2 in grid:\n",
        "            for l3 in grid:\n",
        "                l4 = 1 - (l1+l2+l3)\n",
        "                if l4 < 0:  # invalid combo\n",
        "                    continue\n",
        "                lambdas = (l1, l2, l3, l4)\n",
        "                total_logp = 0.0\n",
        "                for sent in val_set[:max_sent]:\n",
        "                    total_logp += sentence_logprob_interpolated(sent, lambdas)\n",
        "                if total_logp > best_score:\n",
        "                    best_score = total_logp\n",
        "                    best_lambdas = lambdas\n",
        "    return best_lambdas, best_score\n",
        "\n",
        "# -------------------------\n",
        "# Example: run fast search\n",
        "# -------------------------\n",
        "best_lambdas, best_score = grid_search_lambdas_fast(val_set, step=0.2, max_sent=200)\n",
        "print(\"Best λ's (λ1, λ2, λ3, λ4):\", best_lambdas)\n",
        "print(\"Validation log-likelihood:\", best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twsiCdo81Dey",
        "outputId": "a0905619-f5af-46c9-c8b1-22c2a45b006c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best λ's (λ1, λ2, λ3, λ4): (0.6, 0.4, 0.0, 0.0)\n",
            "Validation log-likelihood: -30816.31385418249\n"
          ]
        }
      ]
    }
  ]
}